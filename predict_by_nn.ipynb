{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFT2018 contest - Classification & Fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T15:08:15.854467Z",
     "start_time": "2018-09-28T15:08:15.317837Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "train = pd.read_csv(data_dir+'/train.csv')\n",
    "test = pd.read_csv(data_dir+'/test.csv')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(train.target.value_counts())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def layout(fio):\n",
    "    fio = re.sub('[^А-ЯA-Z Ё]','',fio.upper())\n",
    "    if len(re.sub('[А-Я Ё]','',fio)) == 0:\n",
    "        return 'RU'\n",
    "    elif len(re.sub('[A-Z ]','',fio)) == 0:\n",
    "        return 'EN'\n",
    "    else:\n",
    "        return 'RU-EN'\n",
    "    \n",
    "train['fullname_lay'] = train['fullname'].apply(layout)\n",
    "print(train.shape)\n",
    "print(train.fullname_lay.value_counts())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Все ФИО, которые написаны в двух раскладках - полностью некорректны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train.fullname_lay=='RU-EN'].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ru = train[np.multiply(train.id<=1592433,train.fullname_lay=='RU')].reset_index(drop=True)\n",
    "valid_ru = train[np.multiply(train.id>1592433,train.fullname_lay=='RU')].reset_index(drop=True)\n",
    "print(train_ru.shape,valid_ru.shape)\n",
    "\n",
    "train_en = train[np.multiply(train.id<=1596647,train.fullname_lay=='EN')].reset_index(drop=True)\n",
    "valid_en = train[np.multiply(train.id>1596647,train.fullname_lay=='EN')].reset_index(drop=True)\n",
    "print(train_en.shape,valid_en.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение классификатора 012 на основе предсказаний nn и страны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_ru.target.value_counts())\n",
    "print(valid_en.target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import symspellpy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchlite.torch.learner import Learner\n",
    "from torchlite.torch.learner.cores import ClassifierCore\n",
    "from torchlite.torch.metrics import Metric\n",
    "from torchlite.torch.train_callbacks import TensorboardVisualizerCallback, ModelSaverCallback, ReduceLROnPlateau\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "from nn_correct.loader import FIOLoader\n",
    "from nn_correct.model import CorrectorModel\n",
    "from nn_correct.vectorizer import Vectorizer, ru_idx, en_idx\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "class restore_fio_by_nn_ln:\n",
    "    def __init__(self,model_path,ln_idx):\n",
    "        vect = Vectorizer(ln_idx)\n",
    "        vect_diff = Vectorizer(Vectorizer.make_diff_alphabet(ln_idx))\n",
    "        ref_diff_alphabet = dict((value,key) for key, value in vect_diff.alphabet.items())\n",
    "\n",
    "        model = CorrectorModel(\n",
    "            embedding_size=vect.length,\n",
    "            conv_sizes=[600, 300, 300, 300],\n",
    "            out_size=vect_diff.length,\n",
    "            dropout=0.1,\n",
    "            window=5,\n",
    "            lstm_layers=2,\n",
    "            lstm_size=300\n",
    "        )\n",
    "\n",
    "        ModelSaverCallback.restore_model_from_file(model, model_path, load_with_cpu=True)\n",
    "        model = model.eval()\n",
    "        \n",
    "        self.model=model\n",
    "        self.vect=vect\n",
    "        self.ref_diff_alphabet=ref_diff_alphabet\n",
    "        \n",
    "    def __call__(self,fio):\n",
    "        def restore(fullname, target):\n",
    "                fullname = '#' + fullname + \"#\"\n",
    "                res = []\n",
    "                for src, tg in zip(fullname, target):\n",
    "                    if tg == '':\n",
    "                        res.append(src)\n",
    "                    elif tg == '--':\n",
    "                        pass\n",
    "                    elif len(tg) == 2 and tg[0] == '+':\n",
    "                        res.append(tg[1])\n",
    "                        res.append(src)\n",
    "                    else:\n",
    "                        res.append(tg)\n",
    "                res = ''.join(res)\n",
    "                return res.strip('#')\n",
    "\n",
    "        def restore_fio(just_fio):\n",
    "            s_batch, batch_lengths = self.vect.vect_batch(['#'+just_fio+'#'])\n",
    "            s_batch_torch =torch.from_numpy(s_batch)\n",
    "            prediction = self.model(s_batch_torch, batch_lengths)\n",
    "            diff_idxs = torch.argmax(prediction, dim=1)[0].cpu().numpy()\n",
    "            \n",
    "            prediction_maxpool = prediction.max(dim=2)[0][0].cpu().detach().numpy()\n",
    "            prediction_meanpool = prediction.mean(dim=2)[0].cpu().detach().numpy()\n",
    "            prediction_mmp = np.hstack((prediction_maxpool,prediction_meanpool))\n",
    "\n",
    "            diff = []\n",
    "            for diff_idx in diff_idxs:\n",
    "                diff.append(self.ref_diff_alphabet[diff_idx])\n",
    "\n",
    "            return restore(just_fio,diff), prediction_mmp\n",
    "        \n",
    "        return restore_fio(fio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfbn_ru = restore_fio_by_nn_ln('path to ru model', ru_idx)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "with Pool(5) as pool:\n",
    "    nn_fix_and_prob = pool.map(rfbn_ru, list(valid_ru.fullname))\n",
    "\n",
    "valid_ru['fullname_fix_nn'] = [row[0] for row in nn_fix_and_prob]\n",
    "valid_ru_prob = np.array(tuple(row[1] for row in nn_fix_and_prob))\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "countries = list(train_ru.country) + list(train_en.country)\n",
    "dict_countries = Counter(countries)\n",
    "dict_countries = sorted(dict_countries.items(), key=lambda kv: kv[1], reverse=True)\n",
    "countries_pop = [pair[0] for pair in dict_countries[:40]]\n",
    "\n",
    "def ohe_country(countries,countries_pop):\n",
    "    return np.array([[1 if country==one else 0 for country in countries] for one in countries_pop]).T\n",
    "\n",
    "\n",
    "cl_ru = LogisticRegression(penalty='l2',C=1.25,random_state=42,n_jobs=20)\n",
    "cl_ru.fit(np.hstack((valid_ru_prob,ohe_country(valid_ru.country,countries_pop))),valid_ru.target)\n",
    "pickle.dump(cl_ru, open(data_dir+'/predict_012_ru.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор 012 на вероятностях из nn со страной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cl_ru = LogisticRegression(penalty='l2',C=1.25,random_state=42,n_jobs=20)\n",
    "cross_val_score(cl_ru,np.hstack((valid_ru_prob,ohe_country(valid_ru.country,countries_pop))),valid_ru.target,cv=3,scoring='f1_macro').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfbn_en = restore_fio_by_nn_ln('path to en model',en_idx)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "with Pool(5) as pool:\n",
    "    nn_fix_and_prob = pool.map(rfbn_en, list(valid_en.fullname))\n",
    "\n",
    "valid_en['fullname_fix_nn'] = [row[0] for row in nn_fix_and_prob]\n",
    "valid_en_prob = np.array(tuple(row[1] for row in nn_fix_and_prob))\n",
    "\n",
    "\n",
    "cl_en = LogisticRegression(penalty='l2',C=1.25,random_state=42,n_jobs=20)\n",
    "cl_en.fit(np.hstack((valid_en_prob,ohe_country(valid_en.country,countries_pop))),valid_en.target)\n",
    "pickle.dump(cl_en, open(data_dir+'/predict_012_en.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение классификатора nn-dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ru.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "#список неправильных имен\n",
    "name_err = []\n",
    "for itr in range(len(train)):\n",
    "    if train.target[itr] == 1:\n",
    "        fullname = train.fullname[itr].split(' ')\n",
    "        fullname_true = train.fullname_true[itr].split(' ')\n",
    "        if len(fullname) == len(fullname_true):\n",
    "            for jtr in range(len(fullname)):\n",
    "                if fullname[jtr] != fullname_true[jtr]:\n",
    "                    name_err.append(fullname[jtr]) \n",
    "    elif train.target[itr] == 2:\n",
    "        name_err +=  train.fullname[itr].split(' ')\n",
    "\n",
    "\n",
    "#частотные словари трейна и теста\n",
    "dicts_train = [name for person in train.loc[train.target==0,'fullname'] for name in person.split(' ')] + [name for person in train.loc[train.target==1,'fullname_true'] for name in person.split(' ')]\n",
    "name_freq_train = Counter(dicts_train)\n",
    "\n",
    "dicts_test = [name for person in test.fullname for name in person.split(' ')]\n",
    "name_freq_test = Counter(dicts_test)\n",
    "name_freq_test = {name:freq for name, freq in name_freq_test.items() if freq > 1}\n",
    "\n",
    "\n",
    "#объединение словарей\n",
    "name_freq = defaultdict(int)\n",
    "\n",
    "for name, freq in name_freq_train.items():\n",
    "    name_freq[name] += freq\n",
    "\n",
    "for name in name_err:\n",
    "    if name in name_freq_test:\n",
    "        del name_freq_test[name]\n",
    "    \n",
    "for name, freq in name_freq_test.items():\n",
    "    name_freq[name] += freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import symspellpy\n",
    "symspell = symspellpy.SymSpell()\n",
    "\n",
    "with open(data_dir+'/dictionary.txt', 'w') as f:\n",
    "    for name, freq in name_freq.items():\n",
    "        f.write('{} {}\\n'.format(name, freq))\n",
    "        \n",
    "symspell.load_dictionary(data_dir+'/dictionary.txt', term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(s):\n",
    "    def correct_word(w):\n",
    "        tmp = symspell.lookup(w, symspellpy.Verbosity.CLOSEST)\n",
    "        if len(tmp):\n",
    "            return tmp[0].term.upper()\n",
    "        else:\n",
    "            return w\n",
    "\n",
    "    return ' '.join([correct_word(word) for word in s.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ru['fullname_fix_dict'] = None\n",
    "\n",
    "valid_ru.loc[valid_ru.target==1,'fullname_fix_dict'] = valid_ru.loc[valid_ru.target==1,'fullname'].apply(correct)\n",
    "valid_ru.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_nn = sum(np.multiply(valid_ru.fullname_true==valid_ru.fullname_fix_nn,valid_ru.target==1))/sum(valid_ru.target==1)\n",
    "acc_dict = sum(np.multiply(valid_ru.fullname_true==valid_ru.fullname_fix_dict,valid_ru.target==1))/sum(valid_ru.target==1)\n",
    "acc_nn_and_dict = sum(np.multiply(np.multiply(valid_ru.fullname_true==valid_ru.fullname_fix_nn,valid_ru.fullname_true==valid_ru.fullname_fix_dict),valid_ru.target==1))/sum(valid_ru.target==1)\n",
    "\n",
    "print('nn', acc_nn)\n",
    "print('dict', acc_dict)\n",
    "print('nn and dict', acc_nn_and_dict)\n",
    "print('nn or dict', acc_nn + acc_dict - acc_nn_and_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_feat(fullname, fullname_fix_nn, fullname_fix_dict):\n",
    "    fullname = fullname.split()\n",
    "    fullname_fix_nn = fullname_fix_nn.split()\n",
    "    fullname_fix_dict = fullname_fix_dict.split()\n",
    "    \n",
    "    len_fullname = len(fullname)\n",
    "    len_fullname_fix_nn = len(fullname_fix_nn)\n",
    "    len_fullname_fix_dict = len(fullname_fix_dict)\n",
    "    \n",
    "    diff_fullname = set()\n",
    "    diff_fullname_fix_nn = set()\n",
    "    diff_fullname_fix_dict = set()\n",
    "    \n",
    "    match = set()\n",
    "    \n",
    "    if len_fullname == len_fullname_fix_nn == len_fullname_fix_dict:\n",
    "        for i in range(len_fullname):\n",
    "            if fullname[i] != fullname_fix_nn[i]:\n",
    "                diff_fullname.add(fullname[i])\n",
    "                diff_fullname_fix_nn.add(fullname_fix_nn[i])\n",
    "            else:\n",
    "                match.add(fullname[i])\n",
    "                \n",
    "            if fullname[i] != fullname_fix_dict[i]:\n",
    "                diff_fullname.add(fullname[i])\n",
    "                diff_fullname_fix_dict.add(fullname_fix_dict[i])\n",
    "            else:\n",
    "                match.add(fullname[i])\n",
    "    else:\n",
    "        for i in range(len_fullname):\n",
    "            diff_fullname.add(fullname[i])\n",
    "            match.add(fullname[i])\n",
    "        for i in range(len_fullname_fix_nn):\n",
    "            diff_fullname_fix_nn.add(fullname_fix_nn[i])\n",
    "            match.add(fullname_fix_nn[i])\n",
    "        for i in range(len_fullname_fix_dict):\n",
    "            diff_fullname_fix_dict.add(fullname_fix_dict[i])\n",
    "            match.add(fullname_fix_dict[i])\n",
    "            \n",
    "#     print(diff_fullname,diff_fullname_fix_nn,diff_fullname_fix_dict,match)\n",
    "    \n",
    "    freq_fullname = [name_freq[name] for name in diff_fullname]\n",
    "    freq_fullname_fix_nn = [name_freq[name] for name in diff_fullname_fix_nn]\n",
    "    freq_fullname_fix_dict = [name_freq[name] for name in diff_fullname_fix_dict]\n",
    "    freq_match = [name_freq[name] for name in match]\n",
    "    \n",
    "    f_orig = np.mean(freq_fullname if len(freq_fullname) > 0 else 0)\n",
    "    f_nn = np.mean(freq_fullname_fix_nn if len(freq_fullname_fix_nn) > 0 else 0)\n",
    "    f_dict = np.mean(freq_fullname_fix_dict if len(freq_fullname_fix_dict) > 0 else 0)\n",
    "    \n",
    "    f_other_max = np.max(freq_match if len(freq_match) > 0 else 0)\n",
    "    f_other_min = np.min(freq_match if len(freq_match) > 0 else 0)\n",
    "    f_other_avg = np.mean(freq_match if len(freq_match) > 0 else 0)\n",
    "    \n",
    "    return float(f_orig), float(f_nn), float(f_dict), float(f_other_max), float(f_other_min), float(f_other_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "valid_ru_1 = valid_ru[valid_ru.target==1].reset_index(drop=True)\n",
    "\n",
    "def freq_feat_i(i):\n",
    "    return freq_feat(valid_ru_1.fullname[i], valid_ru_1.fullname_fix_nn[i], valid_ru_1.fullname_fix_dict[i])\n",
    "\n",
    "with Pool(5) as pool:\n",
    "    freq_feat_ru = pool.map(freq_feat_i, range(len(valid_ru_1)))\n",
    "    \n",
    "freq_feat_ru = np.array(freq_feat_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ru_1['matching'] = None\n",
    "\n",
    "valid_ru_1.loc[np.multiply(valid_ru_1.fullname_true==valid_ru_1.fullname_fix_nn, valid_ru_1.fullname_true==valid_ru_1.fullname_fix_dict),'matching'] = 0\n",
    "valid_ru_1.loc[np.multiply(valid_ru_1.fullname_true==valid_ru_1.fullname_fix_nn, valid_ru_1.fullname_true!=valid_ru_1.fullname_fix_dict),'matching'] = 1\n",
    "valid_ru_1.loc[np.multiply(valid_ru_1.fullname_true!=valid_ru_1.fullname_fix_nn, valid_ru_1.fullname_true==valid_ru_1.fullname_fix_dict),'matching'] = 2\n",
    "valid_ru_1.loc[np.multiply(valid_ru_1.fullname_true!=valid_ru_1.fullname_fix_nn, valid_ru_1.fullname_true!=valid_ru_1.fullname_fix_dict),'matching'] = 3\n",
    "\n",
    "valid_ru_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ru_1.matching.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "cl_nndict_ru = XGBClassifier(max_depth=4, learning_rate=0.1, n_estimators=800, n_jobs=20,random_state=42)\n",
    "cl_nndict_ru.fit(freq_feat_ru, valid_ru_1.matching)\n",
    "pickle.dump(cl_nndict_ru, open(data_dir+'/predict_nn_dict_ru.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "        colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "        max_depth=4, min_child_weight=1, missing=None, n_estimators=800,\n",
    "        n_jobs=20, nthread=None, objective='binary:logistic',\n",
    "        random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "        seed=None, silent=True, subsample=1), 0.9409484074334511)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_en['fullname_fix_dict'] = None\n",
    "valid_en.loc[valid_en.target==1,'fullname_fix_dict'] = valid_en.loc[valid_en.target==1,'fullname'].apply(correct)\n",
    "\n",
    "\n",
    "acc_nn = sum(np.multiply(valid_en.fullname_true==valid_en.fullname_fix_nn,valid_en.target==1))/sum(valid_en.target==1)\n",
    "acc_dict = sum(np.multiply(valid_en.fullname_true==valid_en.fullname_fix_dict,valid_en.target==1))/sum(valid_en.target==1)\n",
    "acc_nn_and_dict = sum(np.multiply(np.multiply(valid_en.fullname_true==valid_en.fullname_fix_nn,valid_en.fullname_true==valid_en.fullname_fix_dict),valid_en.target==1))/sum(valid_en.target==1)\n",
    "print('nn', acc_nn)\n",
    "print('dict', acc_dict)\n",
    "print('nn and dict', acc_nn_and_dict)\n",
    "print('nn or dict', acc_nn + acc_dict - acc_nn_and_dict)\n",
    "\n",
    "\n",
    "valid_en_1 = valid_en[valid_en.target==1].reset_index(drop=True)\n",
    "def freq_feat_i(i):\n",
    "    return freq_feat(valid_en_1.fullname[i], valid_en_1.fullname_fix_nn[i], valid_en_1.fullname_fix_dict[i])\n",
    "with Pool(5) as pool:\n",
    "    freq_feat_en = pool.map(freq_feat_i, range(len(valid_en_1)))\n",
    "freq_feat_en = np.array(freq_feat_en)\n",
    "\n",
    "\n",
    "valid_en_1['matching'] = None\n",
    "valid_en_1.loc[np.multiply(valid_en_1.fullname_true==valid_en_1.fullname_fix_nn, valid_en_1.fullname_true==valid_en_1.fullname_fix_dict),'matching'] = 0\n",
    "valid_en_1.loc[np.multiply(valid_en_1.fullname_true==valid_en_1.fullname_fix_nn, valid_en_1.fullname_true!=valid_en_1.fullname_fix_dict),'matching'] = 1\n",
    "valid_en_1.loc[np.multiply(valid_en_1.fullname_true!=valid_en_1.fullname_fix_nn, valid_en_1.fullname_true==valid_en_1.fullname_fix_dict),'matching'] = 2\n",
    "valid_en_1.loc[np.multiply(valid_en_1.fullname_true!=valid_en_1.fullname_fix_nn, valid_en_1.fullname_true!=valid_en_1.fullname_fix_dict),'matching'] = 3\n",
    "print(valid_en_1.matching.value_counts())\n",
    "\n",
    "\n",
    "cl_nndict_en = XGBClassifier(max_depth=4, learning_rate=0.1, n_estimators=800, n_jobs=20,random_state=42)\n",
    "print(cross_val_score(cl_nndict_en,freq_feat_en, valid_en_1.matching,scoring='accuracy',cv=3).mean())\n",
    "cl_nndict_en.fit(freq_feat_en, valid_en_1.matching)\n",
    "pickle.dump(cl_nndict_en, open(data_dir+'/predict_nn_dict_en.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применяем классификаторы 012 для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['fullname_lay'] = test['fullname'].apply(layout)\n",
    "print(test.fullname_lay.value_counts())\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Там где ФИО написаны в двух раскладках - полностью некорректные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test['target'] = None\n",
    "test.loc[test['fullname_lay']=='RU-EN','target'] = 2\n",
    "test['fullname_fix_nn'] = None\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "with Pool(5) as pool:\n",
    "    nn_fix_and_prob = pool.map(rfbn_ru, list(test.loc[test['fullname_lay']=='RU','fullname']))\n",
    "\n",
    "test.loc[test['fullname_lay']=='RU','fullname_fix_nn'] = [row[0] for row in nn_fix_and_prob]\n",
    "test_ru_prob = np.array(tuple(row[1] for row in nn_fix_and_prob))\n",
    "\n",
    "test.loc[test['fullname_lay']=='RU','target'] = cl_ru.predict(np.hstack((test_ru_prob,ohe_country(list(test.loc[test['fullname_lay']=='RU','country']),countries_pop))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "with Pool(5) as pool:\n",
    "    nn_fix_and_prob = pool.map(rfbn_en, list(test.loc[test['fullname_lay']=='EN','fullname']))\n",
    "\n",
    "test.loc[test['fullname_lay']=='EN','fullname_fix_nn'] = [row[0] for row in nn_fix_and_prob]\n",
    "test_en_prob = np.array(tuple(row[1] for row in nn_fix_and_prob))\n",
    "\n",
    "test.loc[test['fullname_lay']=='EN','target'] = cl_en.predict(np.hstack((test_en_prob,ohe_country(list(test.loc[test['fullname_lay']=='EN','country']),countries_pop))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(test.shape)\n",
    "print(test.target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(np.multiply(test.fullname==test.fullname_fix_nn,test.target==1)))\n",
    "print(sum(np.multiply(test.fullname!=test.fullname_fix_nn,test.target==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применям классификаторы nn-dict для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bck = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['fullname_fix_dict'] = None\n",
    "with Pool(5) as pool:\n",
    "    test_fix_dict = pool.map(correct,list(test.loc[test.target==1,'fullname']))\n",
    "test.loc[test.target==1,'fullname_fix_dict'] = test_fix_dict\n",
    "print(1)\n",
    "\n",
    "\n",
    "test_ru_1 = test[np.multiply(test.target==1,test.fullname_lay=='RU')].reset_index(drop=True)\n",
    "def freq_feat_i(i):\n",
    "    return freq_feat(test_ru_1.fullname[i], test_ru_1.fullname_fix_nn[i], test_ru_1.fullname_fix_dict[i])\n",
    "with Pool(5) as pool:\n",
    "    freq_feat_test_ru = pool.map(freq_feat_i, range(len(test_ru_1)))\n",
    "freq_feat_test_ru = np.array(freq_feat_test_ru)\n",
    "print(2)\n",
    "\n",
    "\n",
    "test['matching'] = None\n",
    "test.loc[np.multiply(test.target==1,test.fullname_lay=='RU'),'matching'] = cl_nndict_ru.predict(freq_feat_test_ru)\n",
    "print(test.loc[np.multiply(test.target==1,test.fullname_lay=='RU'),'matching'].value_counts())\n",
    "print(3)\n",
    "\n",
    "\n",
    "test_en_1 = test[np.multiply(test.target==1,test.fullname_lay=='EN')].reset_index(drop=True)\n",
    "def freq_feat_i(i):\n",
    "    return freq_feat(test_en_1.fullname[i], test_en_1.fullname_fix_nn[i], test_en_1.fullname_fix_dict[i])\n",
    "with Pool(5) as pool:\n",
    "    freq_feat_test_en = pool.map(freq_feat_i, range(len(test_en_1)))\n",
    "freq_feat_test_en = np.array(freq_feat_test_en)\n",
    "print(4)\n",
    "\n",
    "\n",
    "test.loc[np.multiply(test.target==1,test.fullname_lay=='EN'),'matching'] = cl_nndict_en.predict(freq_feat_test_en)\n",
    "print(test.loc[np.multiply(test.target==1,test.fullname_lay=='EN'),'matching'].value_counts())\n",
    "print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test.target==1]['matching'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['fullname_true'] = None\n",
    "test.loc[np.multiply(test.target==1,test.matching!=2),'fullname_true'] = test.loc[np.multiply(test.target==1,test.matching!=2),'fullname_fix_nn']\n",
    "test.loc[np.multiply(test.target==1,test.matching==2),'fullname_true'] = test.loc[np.multiply(test.target==1,test.matching==2),'fullname_fix_dict']\n",
    "\n",
    "pd.DataFrame.to_csv(test[['id','target','fullname_true']],data_dir+'/sub_nn_wcountry_wdict_target_fn.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
